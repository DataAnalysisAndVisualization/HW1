{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import numpy as np\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:00:57.726724Z",
     "start_time": "2024-07-02T13:00:43.966959Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "COHERE_API_KEY =\"bCicpvvKIFV8VV9bpC1ITwjYa6rjLeOPrwz6HVkm\"\n",
    "PINECONE_API_KEY = \"198559e6-23b6-49b3-b1ff-acc8ee2e6340\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:00:57.741721Z",
     "start_time": "2024-07-02T13:00:57.730725Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:12.489070Z",
     "start_time": "2024-07-02T13:00:57.745719Z"
    }
   },
   "source": [
    "#First lets write a query for the LLM\n",
    "query = \"How many people served as U.S. president between 1880 and 2000?\"\n",
    "\n",
    "co = cohere.Client(api_key=COHERE_API_KEY)\n",
    "response = co.chat(\n",
    "        model='command-r-plus',\n",
    "        message=query,\n",
    "    )\n",
    "response.text"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There were 21 U.S. presidents who served between 1880 and 2000. They are, in order of their presidency:\\n\\n1. Rutherford B. Hayes (1877-1881)\\n2. James A. Garfield (1881)\\n3. Chester A. Arthur (1881-1885)\\n4. Grover Cleveland (1885-1889)\\n5. Benjamin Harrison (1889-1893)\\n6. Grover Cleveland (1893-1897)\\n7. William McKinley (1897-1901)\\n8. Theodore Roosevelt (1901-1909)\\n9. William Howard Taft (1909-1913)\\n10. Woodrow Wilson (1913-1921)\\n11. Warren G. Harding (1921-1923)\\n1Multiplier-2Calvin Coolidge (1923-1929)\\n13. Herbert Hoover (1929-1933)\\n14. Franklin D. Roosevelt (1933-1945)\\n15. Harry S. Truman (1945-1953)\\n16. Dwight D. Eisenhower (1953-1961)\\n17. John F. Kennedy (1961-1963)\\n18. Lyndon B. Johnson (1963-1969)\\n19. Richard Nixon (1969-1974)\\n20. Gerald Ford (1974-1977)\\n21. Jimmy Carter (1977-1981)\\n22. Ronald Reagan (1981-1989)\\n23. George H.W. Bush (1989-1993)\\n24. Bill Clinton (1993-2001)\\n\\nNote that Grover Cleveland served two non-consecutive terms, so he is counted twice in the list of presidents during this time period.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:22.729068Z",
     "start_time": "2024-07-02T13:01:12.493083Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:25.370070Z",
     "start_time": "2024-07-02T13:01:22.733071Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def load_and_embedd_dataset(\n",
    "        dataset_path: str = 'wikitext',\n",
    "        dataset_name: str = 'wikitext-2-raw-v1',\n",
    "        split: str = 'train',\n",
    "        model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "        text_field: str = 'text',\n",
    "        rec_num: int = 400\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Load a dataset and embedd the text field using a sentence-transformer model\n",
    "    Args:\n",
    "        dataset_name: The name of the dataset to load\n",
    "        split: The split of the dataset to load\n",
    "        model: The model to use for embedding\n",
    "        text_field: The field in the dataset that contains the text\n",
    "        rec_num: The number of records to load and embedd\n",
    "    Returns:\n",
    "        tuple: A tuple containing the dataset and the embeddings\n",
    "    \"\"\"\n",
    "    print(\"Loading and embedding the dataset\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_path, dataset_name, split=split)\n",
    "\n",
    "    # Embed the first `rec_num` rows of the dataset\n",
    "    embeddings = model.encode(dataset[text_field][:rec_num])\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return dataset, embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:27.029071Z",
     "start_time": "2024-07-02T13:01:25.372072Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_path = 'wikitext'\n",
    "dataset_name = 'wikitext-2-raw-v1'\n",
    "\n",
    "dataset, embeddings = load_and_embedd_dataset(\n",
    "    dataset_path=dataset_path,\n",
    "    dataset_name=dataset_name,\n",
    "    rec_num=400,\n",
    "    model=model,\n",
    ")\n",
    "shape = embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:47.731598Z",
     "start_time": "2024-07-02T13:01:27.031070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding the dataset\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "pd_dataset = dataset.to_pandas()\n",
    "pd_dataset.head(7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:47.968599Z",
     "start_time": "2024-07-02T13:01:47.738603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text\n",
       "0                                                   \n",
       "1                     = Valkyria Chronicles III = \\n\n",
       "2                                                   \n",
       "3   Senjō no Valkyria 3 : Unrecorded Chronicles (...\n",
       "4   The game began development in 2010 , carrying...\n",
       "5   It met with positive sales in Japan , and was...\n",
       "6                                                   "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It met with positive sales in Japan , and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"The embeddings shape: {embeddings.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:47.984603Z",
     "start_time": "2024-07-02T13:01:47.971599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embeddings shape: (400, 384)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "def create_pinecone_index(\n",
    "        index_name: str,\n",
    "        dimension: int,\n",
    "        metric: str = 'cosine',\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pinecone index if it does not exist\n",
    "    Args:\n",
    "        index_name: The name of the index\n",
    "        dimension: The dimension of the index\n",
    "        metric: The metric to use for the index\n",
    "    Returns:\n",
    "        Pinecone: A pinecone object which can later be used for upserting vectors and connecting to VectorDBs\n",
    "    \"\"\"\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    print(\"Creating a Pinecone index...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            # Remember! It is crucial that the metric you will use in your VectorDB will also be a metric your embedding\n",
    "            # model works well with!\n",
    "            metric=metric,\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "    print(\"Done!\")\n",
    "    return pc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:48.014597Z",
     "start_time": "2024-07-02T13:01:47.987601Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "INDEX_NAME = 'cnn-dailymail'\n",
    "\n",
    "# Create the vector database\n",
    "# We are passing the index_name and the size of our embeddings\n",
    "pc = create_pinecone_index(INDEX_NAME, shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:48.390600Z",
     "start_time": "2024-07-02T13:01:48.017602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a Pinecone index...\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def upsert_vectors(\n",
    "        index: Pinecone,\n",
    "        embeddings: np.ndarray,\n",
    "        dataset: dict,\n",
    "        text_field: str = 'text',\n",
    "        batch_size: int = 128\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert vectors to a pinecone index\n",
    "    Args:\n",
    "        index: The pinecone index object\n",
    "        embeddings: The embeddings to upsert\n",
    "        dataset: The dataset containing the metadata\n",
    "        batch_size: The batch size to use for upserting\n",
    "    Returns:\n",
    "        An updated pinecone index\n",
    "    \"\"\"\n",
    "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "    shape = embeddings.shape\n",
    "\n",
    "    ids = [str(i) for i in range(shape[0])]\n",
    "    meta = [{text_field: text} for text in dataset[text_field]]\n",
    "\n",
    "    # create list of (id, vector, metadata) tuples to be upserted\n",
    "    to_upsert = list(zip(ids, embeddings, meta))\n",
    "\n",
    "    for i in tqdm(range(0, shape[0], batch_size)):\n",
    "        i_end = min(i + batch_size, shape[0])\n",
    "        index.upsert(vectors=to_upsert[i:i_end])\n",
    "    return index\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:01:48.405601Z",
     "start_time": "2024-07-02T13:01:48.394605Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Upsert the embeddings to the Pinecone index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "index_upserted = upsert_vectors(index, embeddings, dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:02:05.222552Z",
     "start_time": "2024-07-02T13:01:48.408601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.04s/it]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "index.describe_index_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:02:05.432551Z",
     "start_time": "2024-07-02T13:02:05.230553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 400}},\n",
       " 'total_vector_count': 400}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "def augment_prompt(\n",
    "        query: str,\n",
    "        model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "        index=None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Augment the prompt with the top 3 results from the knowledge base\n",
    "    Args:\n",
    "        query: The query to augment\n",
    "        index: The vectorstore object\n",
    "    Returns:\n",
    "        str: The augmented prompt\n",
    "    \"\"\"\n",
    "    results = [float(val) for val in list(model.encode(query))]\n",
    "\n",
    "    # get top 3 results from knowledge base\n",
    "    query_results = index.query(\n",
    "        vector=results,\n",
    "        top_k=3,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )['matches']\n",
    "    text_matches = [match['metadata']['text'] for match in query_results]\n",
    "\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\\n\".join(text_matches)\n",
    "\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "    If the answer is not included in the source knowledge - say that you don't know.\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt, source_knowledge"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:02:07.612549Z",
     "start_time": "2024-07-02T13:02:05.434552Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T13:02:07.627553Z",
     "start_time": "2024-07-02T13:02:07.615551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_models(query):\n",
    "    co = cohere.Client(api_key=COHERE_API_KEY)\n",
    "    response = co.chat(\n",
    "            model='command-r-plus',\n",
    "            message=query,\n",
    "        )\n",
    "    print(\"Original response:\")\n",
    "    print(response.text)\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    augmented_prompt, source_knowledge = augment_prompt(query, model=model, index=index)\n",
    "    response = co.chat(\n",
    "            model='command-r-plus',\n",
    "            message=augmented_prompt,\n",
    "        )\n",
    "    print(\"Augmented response:\")\n",
    "    print(response.text)\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print(\"Source knowledge:\")\n",
    "    print(source_knowledge)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Who sung the opening theme in Valkyria Chronicles III?\"\n",
    "compare_models(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:02:10.210873Z",
     "start_time": "2024-07-02T13:02:07.629552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original response:\n",
      "*Valkyria Chronicles III* is an amazing game with a beautiful opening theme! The song is called “Blue Star” and it is performed by Japanese singer and voice actress, Kanako Kotera.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Augmented response:\n",
      "May'n\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Source knowledge:\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      "\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      " The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally heard about the project , he thought it would be a light tone similar to other Valkyria Chronicles games , but found the themes much darker than expected . An early theme he designed around his original vision of the project was rejected . He redid the main theme about seven times through the music production due to this need to reassess the game . The main theme was initially recorded using orchestra , then Sakimoto removed elements such as the guitar and bass , then adjusted the theme using a synthesizer before redoing segments such as the guitar piece on their own before incorporating them into the theme . The rejected main theme was used as a hopeful tune that played during the game 's ending . The battle themes were designed around the concept of a \" modern battle \" divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of atonality . While Sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar . The guitar was played by Mitsuhiro Ohta , who also arranged several of the later tracks . The game 's opening theme song , \" If You Wish for ... \" ( もしも君が願うのなら , Moshimo Kimi ga Negauno Nara ) , was sung by Japanese singer May 'n . Its theme was the reason soldiers fought , in particular their wish to protect what was precious to them rather than a sense of responsibility or duty . Its lyrics were written by Seiko Fujibayashi , who had worked on May 'n on previous singles . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"When did Raphael Tuck buy four of Barker's 'little drawings'?\"\n",
    "compare_models(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:02:12.409161Z",
     "start_time": "2024-07-02T13:02:10.213875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original response:\n",
      "Raphael Tuck bought four of Barker's \"little drawings\" in 1866.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Augmented response:\n",
      "In 1911, Raphael Tuck & Sons bought four of Barker's \"little drawings.\"\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Source knowledge:\n",
      " In 1911 , Raphael Tuck & Sons bought four of Barker 's \" little drawings \" for half a sovereign , and published them as postcards . In October 1911 , she won second prize in the Croydon Art Society 's poster competition , and shortly afterward was elected the youngest member of the Society . The art critic for the Croydon Advertiser remarked , \" Her drawings show a remarkable freedom of spirit . She has distinct promise . \" \n",
      "\n",
      "\n",
      " Following her father ’ s death in June 1912 , the seventeen @-@ year @-@ old Barker submitted art and poetry to My Magazine , Child ’ s Own , Leading Strings , and Raphael Tuck annuals in an effort to support both her mother and sister . Her sister Dorothy taught kindergarten in two private schools before opening a kindergarten at home . She brought in some money for the family 's support while supervising the household . \n",
      "\n",
      "\n",
      " Barker was equally proficient in watercolour , pen and ink , oils , and pastels . Kate Greenaway and the Pre @-@ Raphaelites were the principal influences on her work . She claimed to paint instinctively and rejected artistic theories . Barker died in 1973 . Though she published Flower Fairy books with spring , summer , and autumn themes , it wasn 't until 1985 that a winter collection was assembled from her remaining work and published posthumously . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"When did Columbus have the best chance of receiving the first overall pick?\"\n",
    "compare_models(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-02T13:02:14.297184Z",
     "start_time": "2024-07-02T13:02:12.411164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original response:\n",
      "2015\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Augmented response:\n",
      "Columbus had the best chance of receiving the first overall pick in the 2012 NHL Entry Draft lottery.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Source knowledge:\n",
      " Finishing with the worst record in the NHL , Columbus had the best chance of receiving the first overall pick in the 2012 draft . With the NHL 's weighted draft lottery the Blue Jackets had a 48 @.@ 2 % chance of drafting first overall . However , the lottery was won by the Edmonton Oilers , who proceeded to leapfrog Columbus and secure the number one draft pick for a third consecutive year . It was the fifth time that the Blue Jackets were dropped one draft position in the franchise 's 12 lottery participations . \n",
      "\n",
      "\n",
      " Two weeks prior to the NHL trade deadline , Columbus announced that unlike earlier in the season , they would listen to trade proposals involving Rick Nash , though they were not actively shopping him . Howson stated that the team was open to all options for improving the team , including trading Nash . Speculation was that in return for Nash the Blue Jackets would ask for a \" combination of young , proven players , high @-@ end prospects and draft picks . \" Leading up to the trade deadline , the Blue Jackets dealt Antoine Vermette to the Phoenix Coyotes for two draft picks and goaltender Curtis McElhinney . Despite being injured at the time , the acquisition of McElhinney was believed to give Columbus the flexibility to trade Curtis Sanford . The following day , on February 23 , Columbus traded Jeff Carter to the Kings . In the deal , Columbus acquired defenseman Jack Johnson and a first @-@ round draft pick ; the team was given the choice of taking the pick in either 2012 or 2013 . At the deadline , Columbus was unable to come to terms on a deal involving Nash , but they did make one more move ; they sent center Samuel Pahlsson to the Vancouver Canucks in exchange for two fourth @-@ round draft picks and minor league defenseman Taylor Ellington . Following the trade deadline , Howson announced that the team had attempted to trade Nash at the player 's request . Nash stated that he had requested the trade after being informed that the franchise was going into another rebuilding phase . He further noted that he felt that he \" could be a huge part of that towards bringing assets in , \" and in his view \" it was the best thing for the team , the organization , and personally for [ his ] career . \" After the personnel changes , the Blue Jackets closed out the month with a three @-@ game losing streak . \n",
      "\n",
      "\n",
      " The 2011 – 12 Columbus Blue Jackets season was the team 's 12th season in the National Hockey League ( NHL ) . The Blue Jackets ' record of 29 – 46 – 7 [ note 1 ] was the worst record in the NHL for 2011 – 12 and the first time in franchise history they finished in last place . It also marked the third straight year that they missed the playoffs . Consequently , they had the best chance to receive the first overall selection in the 2012 NHL Entry Draft lottery , but lost out to the Edmonton Oilers and received the second pick instead . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
